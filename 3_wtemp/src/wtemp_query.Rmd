---
title: "Temperature Trend Data Query"
author: "Alison Appling and Lindsay Carr"
date: "August 28, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r dependencies, include=FALSE}
library(dplyr)
library(ggplot2)
library(feather)
library(scipiper)
library(tidyr)

# setwd('3_wtemp/src') # if running manually rather than knitting the whole document

# sc_retrieve("1_nwisdata/out/inventory/nwis_inventory.rds.ind")
nwis_inv_file="../../1_nwisdata/out/inventory/nwis_inventory.rds"

# sc_retrieve("1_wqpdata/tmp/data/inventory_temperature.feather.ind")
wqp_inv_file="../../1_wqpdata/tmp/data/inventory_temperature.feather"

# options(scipiper.dry_put = TRUE)
# scmake("tasks_1_wqp_inventory.yml")
# scmake("1_wqpdata/tmp/data/partition_temperature.rds.ind", remake_file="tasks_1_wqp_inventory.yml")
# scmake("tasks_1_wqp.yml")
# scmake("tasks_1_wqp_temperature.yml", remake_file="tasks_1_wqp.yml")
# wqp_data_ind <- dir("1_wqpdata/out/data", pattern=".*temperature_[[:digit:]]{3}\\.feather\\.ind", full.names=TRUE)
# wqp_data_files <- sapply(wqp_data_ind, function(wdi) {
#   message(wdi)
#   gd_get(wdi)
# })
wqp_data_files <- dir("../../1_wqpdata/out/data", pattern=".*temperature_[[:digit:]]{3}\\.feather$", full.names=TRUE)
```

## Continuous monitoring sites from NWIS

We'll use an NWIS inventory to learn about continuous water temperature data. 
Eventually it'd be nice to pull down all the NWIS samples, but until then,
we need to take shortcuts to approximately determine which sites have good
continuous monitoring coverage.

Our inventory query took this form:
```{r nwis_inv_eg, eval=FALSE}
dataRetrieval::whatNWISdata(
  siteType=c('ST', 'ST-CA', 'ST-DCH', 'ST-TS'), # any kind of stream
  service='uv', # unit-value data only
  parameterCd='00010' # water temperature
  # we'd include startDt and endDt, but they get ignored by the service behind whatNWISdata
)
```
(We actually requested data one state at a time, looping over all states, but all the other arguments are as above.)

The full inventory takes a while to run, so we just load it from file here:
```{r nwis_inv_load}
nwis_inventory <- readRDS(nwis_inv_file)
```

Unfortunately, the only coverage information we receive is in the `begin_date` and `end_date` columns of the NWIS inventory. 
The `count_nu` column sounds promising but is not an accurate reflection of the
number of observations, or even the number of days with observations - it's a very basic calculation
of `end_date` - `begin_date`. So we'll just use those dates directly as our
best-available source of information.

We need to make an arbitrary choice about how to define "continuous". We'll go with sites where monitoring occurred over some minimum date range, thereby eliminating sites with short periods of record.
To help with the decision of how short is too short, we can consider the typical range (leaving out a few outliers with begin dates before 2007 or end dates in the future):
```{r begin_end_dates}
nwis_inventory %>%
  mutate(date_range = as.numeric(end_date - begin_date, units='days')) %>%
  ggplot(aes(x=date_range)) +
  geom_histogram(binwidth=365.25/4) +
  geom_rug() +
  theme_bw() +
  xlab('Date range (days)') + ylab('Number of sites with date range')
```

We can quantify the relationship between our threshold choice and the number of sites we'll get back as follows:
```{r nwis_threshold_effect}
nwis_site_counts <- data_frame(
  range_threshold = seq(1, as.numeric(4000, units='days'), by=50),
  site_count = sapply(range_threshold, function(range_thresh) {
    nwis_inventory %>%
      filter(end_date - begin_date >= as.difftime(range_thresh, units='days')) %>%
      pull(site_no) %>%
      unique() %>%
      length()
  }))
ggplot(nwis_site_counts, aes(x=range_threshold, y=site_count)) + geom_line() +
  theme_bw() +
  xlab('Date range threshold (days)') + ylab('Number of sites with date range >= threshold')
```

Based on the above plots and some arbitrary decisions, we define "continuous" as sites having monitoring occurring over a 2000-day range.
This seems reasonable to us because the maximum typical range is nearly 4000 days, and the distribution of date ranges among sites below that maximum is fairly uniform. However, thresholds of 1000, 3000, or 3900 could also make sense depending on the needs of the trends analyses.

Now we extract and count site IDs for those sites meeting the date range threshold criterion.
```{r nwis_inv_filter}
range_threshold <- 2000 # choose the threshold
continuous_sites <- nwis_inventory %>%
  filter(end_date - begin_date >= as.difftime(range_threshold, units='days')) %>%
  pull(site_no) %>% # exract site IDs
  unique() # remove duplicates, which typically arise when temp is monitored at multiple depths
length(continuous_sites)
```

This threshold approach is imperfect. We'd rather look at how many days were monitored per year, and with how many data gaps, but we can't get that information from NWIS without pulling the data themselves, which will take a while.

## Discrete sampling sites from WQP

We have already pulled all the available WQP temperature data for 2010-2015, so we can look at temporal coverage more thoroughly for this dataset. We'll start by loading the data from file.
```{r load_wqp}
wqp_data <- bind_rows(lapply(wqp_data_files, function(data_file) {
  dat <- feather::read_feather(data_file)
  if(nrow(dat) > 0) {
    NA_DT <- which(is.na(dat$ActivityStartDateTime))
    if(length(NA_DT) > 0) dat[NA_DT,'ActivityStartDateTime'] <- as.POSIXct(dat$ActivityStartDate[NA_DT])
    dat %>% select(MonitoringLocationIdentifier, ActivityStartDateTime)
  } else {
    data_frame(MonitoringLocationIdentifier='', ActivityStartDateTime=Sys.time()) %>% filter(FALSE)
  }
}))
head(wqp_data)
```

Now we can look at thresholds by number of years at some minimum number of observations per year.
```{r count_peryear}
wqp_peryear_counts <- wqp_data %>%
  mutate(ActivityYear = strftime(ActivityStartDateTime, '%Y')) %>%
  group_by(MonitoringLocationIdentifier, ActivityYear) %>%
  summarize(NumObs = length(ActivityStartDateTime)) %>% 
  ungroup()
wqp_peryear_counts
```

We fix the minimum number of observations at 6 such that only years with 6 or more observations count as "Complete".
```{r count_years}
wqp_year_counts <- wqp_peryear_counts %>%
  filter(NumObs >= 6) %>%
  group_by(MonitoringLocationIdentifier) %>%
  summarize(NumFullYears = length(ActivityYear))
wqp_year_counts
```

Here is the distribution of "Complete" years at each site. For the first round of this analysis, we are working with the time period of 2010 to 2015, so that the maximum number of complete years is just 6. We are working to expand this by pulling down more WQP data.
```{r plot_count_years}
ggplot(wqp_year_counts, aes(x=NumFullYears)) + geom_histogram(binwidth=1) +
  xlab("Number of Complete Years") + ylab("Number of Sites with X Complete Years")
```

```{r wqp_threshold_effect}
wqp_site_counts <- data_frame(
  range_threshold = 1:20,
  site_count = sapply(range_threshold, function(range_thresh) {
    wqp_year_counts %>%
      filter(NumFullYears >= range_thresh) %>%
      pull(MonitoringLocationIdentifier) %>%
      unique() %>%
      length()
  }))
ggplot(wqp_site_counts, aes(x=range_threshold, y=site_count)) + geom_line() +
  theme_bw() +
  xlab('Complete years threshold (years)') + ylab('Number of sites with complete years >= threshold')
```


For today we choose a threshold that only sites with 4 or more complete years count as having adequate discrete-sampling regimes for the trends analysis. This threshold will undoubtedly change when we have pulled in more WQP data (to reflect the higher possible number of complete years) and could also be adjusted to better reflect the needs of the trends analysis.
```{r filter_wqp}
discrete_sites <- wqp_year_counts %>%
  filter(NumFullYears >= 15) %>%
  pull(MonitoringLocationIdentifier)
length(discrete_sites)
```

Of these qualifying discrete sites, only those with a USGS identifier could possibly also be directly, continuously monitored by USGS, so we next restrict our list to those sites. This gets rid of a lot of agency-monitored sites in WQP, though it's possible that some of those are close to USGS monitoring stations and might be matched up with a space-based crosswalk in the future.
```{r usgs_wqp_sites}
usgs_discrete_sites <- gsub('USGS-', '', grep('USGS-', discrete_sites, value=TRUE))
length(usgs_discrete_sites)
```


## Sites with continuous and discrete data

Now we join by USGS ID to find those sites that meet the criteria for both NWIS and WQP data. 
```{r combine_nwis_wqp}
nwis_wqp_sites <- intersect(usgs_discrete_sites, continuous_sites)
length(nwis_wqp_sites)
```

We can plot these sites by looking to WQP for state codes and NWIS for coordinates (NWIS coordinates are more precise than those supplied by WQP, by several digits).

```{r}
site_coords <- dataRetrieval::whatNWISsites(sites=c(usgs_discrete_sites, continuous_sites)) %>%
  mutate(
    Both = site_no %in% usgs_discrete_sites & site_no %in% continuous_sites,
    JustDiscrete = site_no %in% usgs_discrete_sites & !Both,
    JustContinuous = site_no %in% continuous_sites & !Both,
    Type = case_when(Both ~ "Both", JustDiscrete ~ "Discrete", JustContinuous ~ "Continuous")
  )
```

```{r map_utils, include=FALSE}
library(maptools)
library(maps)
library(sp) 
library(rgdal) # required for spTransform

# utility functions for getting and shifting map data
to_sp <- function(...){
  map <- maps::map(..., fill=TRUE, plot = FALSE)
  IDs <- sapply(strsplit(map$names, ":"), function(x) x[1])
  map.sp <- maptools::map2SpatialPolygons(map, IDs=IDs, proj4string=CRS(latlon.string))
  map.sp.t <- spTransform(map.sp, CRS(proj.string))
  return(map.sp.t)
}
shift_sp <- function(sp, scale, shift, rotate = 0, ref=sp, proj.string=NULL, row.names=NULL){
  orig.cent <- rgeos::gCentroid(ref, byid=TRUE)@coords
  scale <- max(apply(bbox(ref), 1, diff)) * scale
  obj <- elide(sp, rotate=rotate, center=orig.cent, bb = bbox(ref))
  ref <- elide(ref, rotate=rotate, center=orig.cent, bb = bbox(ref))
  obj <- elide(obj, scale=scale, center=orig.cent, bb = bbox(ref))
  ref <- elide(ref, scale=scale, center=orig.cent, bb = bbox(ref))
  new.cent <- rgeos::gCentroid(ref, byid=TRUE)@coords
  obj <- elide(obj, shift=shift*10000+c(orig.cent-new.cent))
  if (is.null(proj.string)){
    proj4string(obj) <- proj4string(sp)
  } else {
    proj4string(obj) <- proj.string
  }
  
  if (!is.null(row.names)){
    row.names(obj) <- row.names
  }
  return(obj)
}

# define projections
latlon.string <- '+proj=longlat +datum=WGS84 +no_defs'
proj.string <- '+proj=laea +lat_0=45 +lon_0=-100 +x_0=0 +y_0=0 +a=6370997 +b=6370997 +units=m +no_defs'

# get map data and prepare for shifting
conus <- to_sp('state')
stuff_to_move <- list(
  'alaska' = to_sp("world", "USA:alaska"),
  'hawaii' = to_sp("world", "USA:hawaii"),
  'puerto rico' = to_sp("world", "Puerto Rico")
)
move_variables <- list(
  'alaska' = list(scale=0.33, shift = c(80,-450), rotate=-50),
  'hawaii' = list(scale=1, shift=c(520, -110), rotate=-35),
  'puerto rico' = list(scale=2.5, shift = c(-140, 90), rotate=20)
)

# build states (polygons) and sites (points) iteratively
states <- conus
sites <- cbind(site_coords$dec_long_va, site_coords$dec_lat_va) %>%
  SpatialPointsDataFrame(data=select(site_coords, site_no, station_nm, Type), proj4string = CRS(latlon.string)) %>% 
  spTransform(CRS(proj4string(states)))
for(i in names(move_variables)){
  state_to_move <- stuff_to_move[[i]]
  sites_to_move <- which(!is.na(sp::over(sites, state_to_move)))
  
  shifted_states <- do.call(
    shift_sp,
    c(sp = state_to_move, 
      move_variables[[i]],  
      proj.string = proj4string(conus),
      row.names = i))
  states <- rbind(shifted_states, states, makeUniqueIDs = TRUE)
  
  if(length(sites_to_move) > 0) {
    shifted_sites <- do.call(
      shift_sp,
      c(sp = SpatialPoints(sites[sites_to_move, ]),
        move_variables[[i]],
        proj.string = proj4string(conus),
        ref=state_to_move)) %>%
      SpatialPointsDataFrame(data=sites[sites_to_move,]@data, match.ID=FALSE)
    # Warning we're suppressing: In SpatialPointsDataFrame(sp, df, coords.nrs =
    # dots[[1]]@coords.nrs) : forming a SpatialPointsDataFrame based on maching
    # IDs, not on record order. Use match.ID = FALSE to match on record order
    sites <- suppressWarnings(rbind(sites[-sites_to_move, ], shifted_sites))
  }
}
```

```{r map, fig.width=6, fig.height=4}
library(ggplot2)
g <- ggplot() +
  geom_polygon(
    data = states, 
    aes(x = long, y = lat, group = group),
    fill = "grey90",
    alpha = 0.5, color = "white") +
  geom_point(
    data = filter(as.data.frame(sites), Type != 'Both'), 
    aes(x = coords.x1, y=coords.x2, color=Type),
    size = 1.4, alpha=0.8) +
  geom_point(
    data = filter(as.data.frame(sites), Type == 'Both'),
    aes(x = coords.x1, y=coords.x2, color=Type),
    size = 1.4, alpha=0.8) +
  coord_fixed() +
  theme_minimal() +
  theme(panel.grid = element_blank(),
        axis.text = element_blank(),
        axis.title = element_blank(),
        legend.position = 'bottom')
ggsave('map.png', plot=g, width=9, height=6.5, device='png', dpi=450)
g
```

```{r totals_table, results='asis'}
data_frame(
  Continuous = length(continuous_sites),
  Discrete = length(usgs_discrete_sites),
  Both = length(nwis_wqp_sites)) %>%
  knitr::kable(caption="Counts of USGS sites with continuous and/or discrete monitoring above our thresholds")
```


```{r states_table, results='asis'}
for(st in names(states)) {
  state <- states[st]
  sites_in_state <- which(!is.na(sp::over(sites, state)))
  sites[sites_in_state, 'State'] <- st
}
as_data_frame(sites) %>%
  group_by(State, Type) %>%
  summarize(NumSites = length(site_no)) %>%
  tidyr::spread(Type, NumSites, fill=0) %>%
  knitr::kable(caption="Counts of USGS sites by monitoring type[s] by state")
```
